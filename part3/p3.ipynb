{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yubin\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel # Import TensorFlow specific model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import sys\n",
    "import time # To measure encoding time\n",
    "\n",
    "# Add path to your dataloader\n",
    "sys.path.append('../dataloader')\n",
    "from dataloader import daigtv2_loader # Assuming this function loads your data into a pandas DataFrame\n",
    "sys.path.append('../part3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded. Found 44868 texts.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "path_to_folder = \"../../\" # Adjust if your dataloader needs a different path\n",
    "TEST_SIZE = 0.2 # Percentage of data to use for testing\n",
    "RANDOM_STATE = 42 # For reproducible splits\n",
    "TRANSFORMER_MODEL_NAME = 'bert-base-uncased' # Using a standard BERT model\n",
    "BATCH_SIZE_ENCODING = 32 # Batch size for processing text through the transformer (adjust based on GPU memory)\n",
    "# TensorFlow automatically uses GPU if available and configured\n",
    "\n",
    "# --- 1. Load your data ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = daigtv2_loader(path_to_folder)\n",
    "    texts = df['text'].values\n",
    "    labels = df['label'].values\n",
    "    print(f\"Data loaded. Found {len(texts)} texts.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    sys.exit(\"Exiting due to data loading error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and test sets (80% train, 20% test)...\n",
      "Train set size: 35894\n",
      "Test set size: 8974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Split data ---\n",
    "print(f\"Splitting data into train and test sets ({1-TEST_SIZE:.0%} train, {TEST_SIZE:.0%} test)...\")\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels # Stratify to maintain label distribution\n",
    ")\n",
    "print(f\"Train set size: {len(X_train_text)}\")\n",
    "print(f\"Test set size: {len(X_test_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pre-trained TensorFlow Transformer model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Transformer model and tokenizer loaded successfully.\n",
      "TensorFlow version: 2.10.1\n",
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Load Pre-trained Transformer Tokenizer and TensorFlow Model ---\n",
    "print(f\"\\nLoading pre-trained TensorFlow Transformer model: {TRANSFORMER_MODEL_NAME}\")\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "    # Use TFBertModel for the TensorFlow version\n",
    "    model_transformer = TFBertModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "    print(\"TensorFlow Transformer model and tokenizer loaded successfully.\")\n",
    "    # TensorFlow automatically manages device placement (GPU/CPU)\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading TensorFlow Transformer model or tokenizer: {e}\")\n",
    "    sys.exit(\"Exiting due to Transformer loading error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define Text Encoding Function (TensorFlow) ---\n",
    "def encode_texts_tf(texts, tokenizer, model, max_length=512, batch_size=32):\n",
    "    \"\"\"\n",
    "    Encodes a list of texts into fixed-size vectors using a TensorFlow Transformer model.\n",
    "    Uses the [CLS] token embedding as the sentence representation.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    print(f\"Encoding {num_texts} texts in batches of {batch_size} using TensorFlow...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process texts in batches\n",
    "    for i in range(0, num_texts, batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenize the batch, returning TensorFlow tensors\n",
    "        # Explicitly convert NumPy array slice to a Python list\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts.tolist(), # <--- FIX: Convert NumPy slice to list\n",
    "            padding=True,          # Pad sequences to the longest in the batch\n",
    "            truncation=True,       # Truncate sequences longer than max_length\n",
    "            max_length=max_length, # Maximum length of sequences\n",
    "            return_tensors='tf'    # Return TensorFlow tensors\n",
    "        )\n",
    "\n",
    "        # Get embeddings from the model\n",
    "        # No need for torch.no_grad(), TensorFlow handles this in inference mode\n",
    "        outputs = model(encoded_input)\n",
    "\n",
    "        # Extract the embedding of the [CLS] token (usually the first token)\n",
    "        # outputs.last_hidden_state has shape (batch_size, sequence_length, hidden_size)\n",
    "        # We take the first token ([CLS]) for all items in the batch: outputs.last_hidden_state[:, 0, :]\n",
    "        # Convert TensorFlow tensor to NumPy array\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "        embeddings.append(cls_embeddings)\n",
    "\n",
    "        # Print progress\n",
    "        if (i + batch_size) % (batch_size * 10) == 0: # Print progress every 10 batches\n",
    "             print(f\"Encoded {i + batch_size}/{num_texts} texts...\")\n",
    "\n",
    "\n",
    "    # Concatenate embeddings from all batches\n",
    "    all_embeddings = np.vstack(embeddings)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Encoding complete. Total time: {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Shape of generated embeddings: {all_embeddings.shape}\")\n",
    "\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 35894 texts in batches of 32 using TensorFlow...\n",
      "Encoded 320/35894 texts...\n",
      "Encoded 640/35894 texts...\n",
      "Encoded 960/35894 texts...\n",
      "Encoded 1280/35894 texts...\n",
      "Encoded 1600/35894 texts...\n",
      "Encoded 1920/35894 texts...\n",
      "Encoded 2240/35894 texts...\n",
      "Encoded 2560/35894 texts...\n",
      "Encoded 2880/35894 texts...\n",
      "Encoded 3200/35894 texts...\n",
      "Encoded 3520/35894 texts...\n",
      "Encoded 3840/35894 texts...\n",
      "Encoded 4160/35894 texts...\n",
      "Encoded 4480/35894 texts...\n",
      "Encoded 4800/35894 texts...\n",
      "Encoded 5120/35894 texts...\n",
      "Encoded 5440/35894 texts...\n",
      "Encoded 5760/35894 texts...\n",
      "Encoded 6080/35894 texts...\n",
      "Encoded 6400/35894 texts...\n",
      "Encoded 6720/35894 texts...\n",
      "Encoded 7040/35894 texts...\n",
      "Encoded 7360/35894 texts...\n",
      "Encoded 7680/35894 texts...\n",
      "Encoded 8000/35894 texts...\n",
      "Encoded 8320/35894 texts...\n",
      "Encoded 8640/35894 texts...\n",
      "Encoded 8960/35894 texts...\n",
      "Encoded 9280/35894 texts...\n",
      "Encoded 9600/35894 texts...\n",
      "Encoded 9920/35894 texts...\n",
      "Encoded 10240/35894 texts...\n",
      "Encoded 10560/35894 texts...\n",
      "Encoded 10880/35894 texts...\n",
      "Encoded 11200/35894 texts...\n",
      "Encoded 11520/35894 texts...\n",
      "Encoded 11840/35894 texts...\n",
      "Encoded 12160/35894 texts...\n",
      "Encoded 12480/35894 texts...\n",
      "Encoded 12800/35894 texts...\n",
      "Encoded 13120/35894 texts...\n",
      "Encoded 13440/35894 texts...\n",
      "Encoded 13760/35894 texts...\n",
      "Encoded 14080/35894 texts...\n",
      "Encoded 14400/35894 texts...\n",
      "Encoded 14720/35894 texts...\n",
      "Encoded 15040/35894 texts...\n",
      "Encoded 15360/35894 texts...\n",
      "Encoded 15680/35894 texts...\n",
      "Encoded 16000/35894 texts...\n",
      "Encoded 16320/35894 texts...\n",
      "Encoded 16640/35894 texts...\n",
      "Encoded 16960/35894 texts...\n",
      "Encoded 17280/35894 texts...\n",
      "Encoded 17600/35894 texts...\n",
      "Encoded 17920/35894 texts...\n",
      "Encoded 18240/35894 texts...\n",
      "Encoded 18560/35894 texts...\n",
      "Encoded 18880/35894 texts...\n",
      "Encoded 19200/35894 texts...\n",
      "Encoded 19520/35894 texts...\n",
      "Encoded 19840/35894 texts...\n",
      "Encoded 20160/35894 texts...\n",
      "Encoded 20480/35894 texts...\n",
      "Encoded 20800/35894 texts...\n",
      "Encoded 21120/35894 texts...\n",
      "Encoded 21440/35894 texts...\n",
      "Encoded 21760/35894 texts...\n",
      "Encoded 22080/35894 texts...\n",
      "Encoded 22400/35894 texts...\n",
      "Encoded 22720/35894 texts...\n",
      "Encoded 23040/35894 texts...\n",
      "Encoded 23360/35894 texts...\n",
      "Encoded 23680/35894 texts...\n",
      "Encoded 24000/35894 texts...\n",
      "Encoded 24320/35894 texts...\n",
      "Encoded 24640/35894 texts...\n",
      "Encoded 24960/35894 texts...\n",
      "Encoded 25280/35894 texts...\n",
      "Encoded 25600/35894 texts...\n",
      "Encoded 25920/35894 texts...\n",
      "Encoded 26240/35894 texts...\n",
      "Encoded 26560/35894 texts...\n",
      "Encoded 26880/35894 texts...\n",
      "Encoded 27200/35894 texts...\n",
      "Encoded 27520/35894 texts...\n",
      "Encoded 27840/35894 texts...\n",
      "Encoded 28160/35894 texts...\n",
      "Encoded 28480/35894 texts...\n",
      "Encoded 28800/35894 texts...\n",
      "Encoded 29120/35894 texts...\n",
      "Encoded 29440/35894 texts...\n",
      "Encoded 29760/35894 texts...\n",
      "Encoded 30080/35894 texts...\n",
      "Encoded 30400/35894 texts...\n",
      "Encoded 30720/35894 texts...\n",
      "Encoded 31040/35894 texts...\n",
      "Encoded 31360/35894 texts...\n",
      "Encoded 31680/35894 texts...\n",
      "Encoded 32000/35894 texts...\n",
      "Encoded 32320/35894 texts...\n",
      "Encoded 32640/35894 texts...\n",
      "Encoded 32960/35894 texts...\n",
      "Encoded 33280/35894 texts...\n",
      "Encoded 33600/35894 texts...\n",
      "Encoded 33920/35894 texts...\n",
      "Encoded 34240/35894 texts...\n",
      "Encoded 34560/35894 texts...\n",
      "Encoded 34880/35894 texts...\n",
      "Encoded 35200/35894 texts...\n",
      "Encoded 35520/35894 texts...\n",
      "Encoded 35840/35894 texts...\n",
      "Encoding complete. Total time: 1704.48 seconds.\n",
      "Shape of generated embeddings: (35894, 768)\n",
      "Encoding 8974 texts in batches of 32 using TensorFlow...\n",
      "Encoded 320/8974 texts...\n",
      "Encoded 640/8974 texts...\n",
      "Encoded 960/8974 texts...\n",
      "Encoded 1280/8974 texts...\n",
      "Encoded 1600/8974 texts...\n",
      "Encoded 1920/8974 texts...\n",
      "Encoded 2240/8974 texts...\n",
      "Encoded 2560/8974 texts...\n",
      "Encoded 2880/8974 texts...\n",
      "Encoded 3200/8974 texts...\n",
      "Encoded 3520/8974 texts...\n",
      "Encoded 3840/8974 texts...\n",
      "Encoded 4160/8974 texts...\n",
      "Encoded 4480/8974 texts...\n",
      "Encoded 4800/8974 texts...\n",
      "Encoded 5120/8974 texts...\n",
      "Encoded 5440/8974 texts...\n",
      "Encoded 5760/8974 texts...\n",
      "Encoded 6080/8974 texts...\n",
      "Encoded 6400/8974 texts...\n",
      "Encoded 6720/8974 texts...\n",
      "Encoded 7040/8974 texts...\n",
      "Encoded 7360/8974 texts...\n",
      "Encoded 7680/8974 texts...\n",
      "Encoded 8000/8974 texts...\n",
      "Encoded 8320/8974 texts...\n",
      "Encoded 8640/8974 texts...\n",
      "Encoded 8960/8974 texts...\n",
      "Encoding complete. Total time: 425.35 seconds.\n",
      "Shape of generated embeddings: (8974, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Encode Training and Test Data ---\n",
    "# BERT models typically have a max sequence length of 512 tokens\n",
    "BERT_MAX_LEN = 512\n",
    "\n",
    "X_train_encoded = encode_texts_tf(\n",
    "    X_train_text,\n",
    "    tokenizer,\n",
    "    model_transformer,\n",
    "    max_length=BERT_MAX_LEN,\n",
    "    batch_size=BATCH_SIZE_ENCODING\n",
    ")\n",
    "\n",
    "X_test_encoded = encode_texts_tf(\n",
    "    X_test_text,\n",
    "    tokenizer,\n",
    "    model_transformer,\n",
    "    max_length=BERT_MAX_LEN,\n",
    "    batch_size=BATCH_SIZE_ENCODING\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CatBoost Classifier...\n",
      "0:\tlearn: 0.9227726\ttest: 0.9174281\tbest: 0.9174281 (0)\ttotal: 379ms\tremaining: 6m 18s\n",
      "100:\tlearn: 0.9848164\ttest: 0.9818364\tbest: 0.9818364 (100)\ttotal: 15.4s\tremaining: 2m 17s\n",
      "200:\tlearn: 0.9905277\ttest: 0.9857366\tbest: 0.9857366 (196)\ttotal: 28.9s\tremaining: 1m 54s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9865166035\n",
      "bestIteration = 235\n",
      "\n",
      "Shrink model to first 236 iterations.\n",
      "CatBoost training complete. Total time: 40.94 seconds.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Train CatBoost Classifier ---\n",
    "print(\"\\nTraining CatBoost Classifier...\")\n",
    "\n",
    "# CatBoost works well with default parameters, but you can tune them\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000, # Number of boosting iterations (trees)\n",
    "    learning_rate=0.05,\n",
    "    loss_function='Logloss', # For binary classification\n",
    "    eval_metric='Accuracy',\n",
    "    random_seed=RANDOM_STATE,\n",
    "    verbose=100, # Print progress every 100 iterations\n",
    "    early_stopping_rounds=50 # Stop if validation metric doesn't improve for 50 rounds\n",
    ")\n",
    "\n",
    "# Create CatBoost Pool objects (optional but can be useful)\n",
    "# Features are the encoded embeddings (numerical)\n",
    "# Labels are the target labels\n",
    "train_pool = Pool(data=X_train_encoded, label=y_train)\n",
    "test_pool = Pool(data=X_test_encoded, label=y_test) # Use test set as validation set\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "catboost_model.fit(train_pool, eval_set=test_pool)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"CatBoost training complete. Total time: {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating CatBoost Model on the test set...\n",
      "Test Accuracy: 0.9865\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      5474\n",
      "           1       0.98      0.98      0.98      3500\n",
      "\n",
      "    accuracy                           0.99      8974\n",
      "   macro avg       0.99      0.99      0.99      8974\n",
      "weighted avg       0.99      0.99      0.99      8974\n",
      "\n",
      "\n",
      "Method 3 (TensorFlow) execution finished.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Evaluate CatBoost Model ---\n",
    "print(\"\\nEvaluating CatBoost Model on the test set...\")\n",
    "\n",
    "# Predict class labels (0 or 1)\n",
    "y_pred = catboost_model.predict(X_test_encoded)\n",
    "\n",
    "# Predict probabilities (if needed)\n",
    "# y_pred_proba = catboost_model.predict_proba(X_test_encoded)[:, 1] # Get probability for the positive class\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print Classification Report (includes precision, recall, f1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nMethod 3 (TensorFlow) execution finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model components...\n",
      "TensorFlow Transformer model saved to: ./saved_transformer_model_tf\n",
      "Transformer tokenizer saved to: ./saved_transformer_tokenizer_tf\n",
      "CatBoost model saved to: ./saved_catboost_model_tf.cbm\n",
      "Model configuration saved to: ./model_config_tf.json\n",
      "Saving complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "# --- 8. Save Model Components ---\n",
    "print(\"\\nSaving model components...\")\n",
    "\n",
    "# Define directories/paths for saving\n",
    "save_dir_transformer = './saved_transformer_model_tf' # Added _tf to distinguish from potential PyTorch saves\n",
    "save_dir_tokenizer = './saved_transformer_tokenizer_tf' # Added _tf\n",
    "save_path_catboost = './saved_catboost_model_tf.cbm' # Added _tf\n",
    "save_path_config = './model_config_tf.json' # Added _tf\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(save_dir_transformer, exist_ok=True)\n",
    "os.makedirs(save_dir_tokenizer, exist_ok=True)\n",
    "\n",
    "# Save the Transformer model (TensorFlow version)\n",
    "try:\n",
    "    model_transformer.save_pretrained(save_dir_transformer)\n",
    "    print(f\"TensorFlow Transformer model saved to: {save_dir_transformer}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving TensorFlow Transformer model: {e}\")\n",
    "\n",
    "# Save the Transformer tokenizer\n",
    "try:\n",
    "    tokenizer.save_pretrained(save_dir_tokenizer)\n",
    "    print(f\"Transformer tokenizer saved to: {save_dir_tokenizer}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Transformer tokenizer: {e}\")\n",
    "\n",
    "# Save the CatBoost model\n",
    "try:\n",
    "    catboost_model.save_model(save_path_catboost)\n",
    "    print(f\"CatBoost model saved to: {save_path_catboost}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving CatBoost model: {e}\")\n",
    "\n",
    "# Save configuration needed for loading\n",
    "config_data = {\n",
    "    'transformer_model_name': TRANSFORMER_MODEL_NAME,\n",
    "    'max_length': BERT_MAX_LEN, # Using BERT_MAX_LEN which is MAX_LEN for BERT\n",
    "    'transformer_model_dir': save_dir_transformer, # Store the saved directory paths\n",
    "    'transformer_tokenizer_dir': save_dir_tokenizer,\n",
    "    'catboost_model_path': save_path_catboost\n",
    "}\n",
    "try:\n",
    "    with open(save_path_config, 'w') as f:\n",
    "        json.dump(config_data, f, indent=4)\n",
    "    print(f\"Model configuration saved to: {save_path_config}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving configuration: {e}\")\n",
    "\n",
    "print(\"Saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 2 texts in batches of 32 using TensorFlow...\n",
      "Encoding complete. Total time: 0.44 seconds.\n",
      "Shape of generated embeddings: (2, 768)\n",
      "\n",
      "Predictions for new texts (0 or 1):\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#--- How to use for new predictions ---\n",
    "#Assuming you have a list of new texts called `new_texts`\n",
    "if tokenizer and model_transformer and catboost_model:\n",
    "    new_texts = np.array([\"I personally have a different argument as to why the electoral college is a good thing. My argument is more about trying to refocus what the office of the President is supposed to be about - the representative of the many states for foreign relations and treaties and as the check on Congress.\\n\\nThe president continues to be regarded as the \\\"most powerful person in the world\\\" which elevates the office to almost monarch-like reverence. The problem with this is it excludes Congress as to where the power is and should be. The notion to change to popular vote gives even more power to what should be a weak executive. A strong(er) executive opens the door to more authoritarian figures to just simply appeal to a cult of personality.\\n\\nSo, the EC is the states voting for their representative. This is it's true purpose and why it should continue IMO.\",\n",
    "    \"Dear Senator,\\n\\nI am writing to you today to express my strong support for abolishing the Electoral College and electing the President by popular vote. I believe that this is the only way to ensure that every American's vote counts and that our elections are truly representative of the will of the people.\\n\\nThe Electoral College is a system that was devised over 200 years ago, when the United States was a very different country. At the time, it was believed that the Electoral College would help to protect the interests of smaller states against the larger states. However, the Electoral College has become increasingly outdated and irrelevant in the 21st century.\\n\\nOne of the biggest problems with the Electoral College is that it gives too much power to a small number of states. In the 2016 election, for example, Donald Trump won the Electoral College despite losing the popular vote by nearly three million votes. This is because Trump won a majority of the electoral votes in a small number of swing states, such as Pennsylvania, Michigan, and Wisconsin.\\n\\nThis system is unfair to the voters in the states that Trump lost. Their votes were essentially ignored, and they had no say in who became President. This is not how a democracy should work.\\n\\nAnother problem with the Electoral College is that it encourages candidates to focus on a small number of swing states. In the 2016 election, for example, Trump spent very little time campaigning in states that he was sure to win, such as California and Texas. Instead, he focused all of his attention on the swing states, where he knew that the election would be decided.\\n\\nThis is not a good way to run a presidential election. Candidates should be campaigning all over the country, not just in a handful of swing states. This is the only way to ensure that all Americans have a voice in the election.\\n\\nI urge you to support legislation that would abolish the Electoral College and elect the President by popular vote. This is the only way to ensure that our elections are truly fair and representative of the will of the people.\\n\\nThank you for your time and consideration.\\n\\nSincerely,\\n\\n[Your Name]\"])\n",
    "    # Use the TensorFlow encoding function\n",
    "    new_encoded_X = encode_texts_tf(new_texts, tokenizer, model_transformer, max_length=BERT_MAX_LEN, batch_size=BATCH_SIZE_ENCODING)\n",
    "    new_predictions = catboost_model.predict(new_encoded_X)\n",
    "    print(\"\\nPredictions for new texts (0 or 1):\")\n",
    "    print(new_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35894,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
