{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import sys\n",
    "sys.path.append('../dataloader')\n",
    "from dataloader import daigtv2_loader\n",
    "sys.path.append('../part1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- 1. Load your data --------\n",
    "# Replace with your actual data-loading code.\n",
    "path_to_folder = \"../../\"\n",
    "\n",
    "df = daigtv2_loader(path_to_folder)\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 2. Tokenize and pad --------\n",
    "MAX_VOCAB = 20000\n",
    "MAX_LEN   = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(MAX_VOCAB, len(word_index)) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- 3. Load pretrained FastText vectors --------\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.random.normal(\n",
    "    size=(vocab_size, EMBEDDING_DIM)\n",
    ").astype(np.float32)\n",
    "\n",
    "def build_embedding_matrix(\n",
    "    vec_file: str,\n",
    "    word_index: dict,\n",
    "    vocab_size: int,\n",
    "    embedding_dim: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads `vec_file` (FastText .vec) line by line,\n",
    "    and fills rows in embedding_matrix for words in word_index.\n",
    "    \"\"\"\n",
    "    with io.open(vec_file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n",
    "        header = fin.readline()  # e.g. \"1000000 300\"\n",
    "        for line in fin:\n",
    "            parts = line.rstrip().split(' ')\n",
    "            word = parts[0]\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                if idx < vocab_size:\n",
    "                    vect = np.asarray(parts[1:], dtype=np.float32)\n",
    "                    if vect.shape[0] == embedding_dim:\n",
    "                        embedding_matrix[idx] = vect\n",
    "    return embedding_matrix\n",
    "\n",
    "# Point this to wherever you downloaded “wiki-news-300d-1M.vec”\n",
    "VEC_FILE = '../../wiki-news-300d-1M.vec'\n",
    "embedding_matrix = build_embedding_matrix(\n",
    "    VEC_FILE, word_index, vocab_size, EMBEDDING_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 500, 300)          6000300   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 256)              439296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,439,853\n",
      "Trainable params: 439,553\n",
      "Non-trainable params: 6,000,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1262/1262 [==============================] - 123s 95ms/step - loss: 0.2959 - accuracy: 0.8842 - val_loss: 0.4791 - val_accuracy: 0.8130 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1262/1262 [==============================] - 118s 94ms/step - loss: 0.1637 - accuracy: 0.9460 - val_loss: 0.5684 - val_accuracy: 0.8344 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1262/1262 [==============================] - 118s 94ms/step - loss: 0.1879 - accuracy: 0.9286 - val_loss: 0.3973 - val_accuracy: 0.8569 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1262/1262 [==============================] - 119s 94ms/step - loss: 0.1442 - accuracy: 0.9490 - val_loss: 0.4936 - val_accuracy: 0.8378 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1262/1262 [==============================] - 119s 94ms/step - loss: 0.1215 - accuracy: 0.9591 - val_loss: 0.5316 - val_accuracy: 0.8346 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1262/1262 [==============================] - 119s 95ms/step - loss: 0.0723 - accuracy: 0.9767 - val_loss: 0.4934 - val_accuracy: 0.8674 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- 4. Define the Keras model --------\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_LEN,\n",
    "        trainable=False,    # freeze pre-trained vectors\n",
    "    ),\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "# -------- 5. Train --------\n",
    "history = model.fit(\n",
    "    X,\n",
    "    labels,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 500, 300)          6000300   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 128)               219648    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,220,077\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 6,000,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1262/1262 [==============================] - 67s 52ms/step - loss: 0.4216 - accuracy: 0.8037 - val_loss: 0.6024 - val_accuracy: 0.6503 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1262/1262 [==============================] - 65s 51ms/step - loss: 0.1623 - accuracy: 0.9494 - val_loss: 1.1417 - val_accuracy: 0.6960 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1262/1262 [==============================] - 65s 52ms/step - loss: 0.1180 - accuracy: 0.9643 - val_loss: 0.2609 - val_accuracy: 0.8982 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1262/1262 [==============================] - 66s 52ms/step - loss: 0.0759 - accuracy: 0.9779 - val_loss: 0.6112 - val_accuracy: 0.7938 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1262/1262 [==============================] - 66s 52ms/step - loss: 0.0571 - accuracy: 0.9836 - val_loss: 0.4877 - val_accuracy: 0.8226 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1262/1262 [==============================] - 65s 52ms/step - loss: 0.0394 - accuracy: 0.9880 - val_loss: 1.1515 - val_accuracy: 0.7025 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- 4. Define the Keras model --------\n",
    "model2 = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_LEN,\n",
    "        trainable=False,    # freeze pre-trained vectors\n",
    "    ),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "# callbacks\n",
    "early_stopping2 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint2 = ModelCheckpoint(\n",
    "    'best_model2.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr2 = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "# -------- 5. Train --------\n",
    "history = model2.fit(\n",
    "    X,\n",
    "    labels,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping2, model_checkpoint2, reduce_lr2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore best weights to each model\n",
    "model.load_weights('best_model.h5')\n",
    "model2.load_weights('best_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Loss: 0.2839, Accuracy: 0.8899\n",
      "Model 2 - Loss: 0.3750, Accuracy: 0.8730\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "loss, accuracy = model.evaluate(X, labels, verbose=0)\n",
    "print(f\"Model 1 - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "loss2, accuracy2 = model2.evaluate(X, labels, verbose=0)\n",
    "print(f\"Model 2 - Loss: {loss2:.4f}, Accuracy: {accuracy2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Human text prediction: 0.4908\n",
      "AI text prediction: 0.9684\n"
     ]
    }
   ],
   "source": [
    "# test specific examples (human from reddit)\n",
    "human_text =  \"I personally have a different argument as to why the electoral college is a good thing. My argument is more about trying to refocus what the office of the President is supposed to be about - the representative of the many states for foreign relations and treaties and as the check on Congress.\\n\\nThe president continues to be regarded as the \\\"most powerful person in the world\\\" which elevates the office to almost monarch-like reverence. The problem with this is it excludes Congress as to where the power is and should be. The notion to change to popular vote gives even more power to what should be a weak executive. A strong(er) executive opens the door to more authoritarian figures to just simply appeal to a cult of personality.\\n\\nSo, the EC is the states voting for their representative. This is it's true purpose and why it should continue IMO.\"\n",
    "\n",
    "# gemini generated text\n",
    "ai_text = \"Dear Senator,\\n\\nI am writing to you today to express my strong support for abolishing the Electoral College and electing the President by popular vote. I believe that this is the only way to ensure that every American's vote counts and that our elections are truly representative of the will of the people.\\n\\nThe Electoral College is a system that was devised over 200 years ago, when the United States was a very different country. At the time, it was believed that the Electoral College would help to protect the interests of smaller states against the larger states. However, the Electoral College has become increasingly outdated and irrelevant in the 21st century.\\n\\nOne of the biggest problems with the Electoral College is that it gives too much power to a small number of states. In the 2016 election, for example, Donald Trump won the Electoral College despite losing the popular vote by nearly three million votes. This is because Trump won a majority of the electoral votes in a small number of swing states, such as Pennsylvania, Michigan, and Wisconsin.\\n\\nThis system is unfair to the voters in the states that Trump lost. Their votes were essentially ignored, and they had no say in who became President. This is not how a democracy should work.\\n\\nAnother problem with the Electoral College is that it encourages candidates to focus on a small number of swing states. In the 2016 election, for example, Trump spent very little time campaigning in states that he was sure to win, such as California and Texas. Instead, he focused all of his attention on the swing states, where he knew that the election would be decided.\\n\\nThis is not a good way to run a presidential election. Candidates should be campaigning all over the country, not just in a handful of swing states. This is the only way to ensure that all Americans have a voice in the election.\\n\\nI urge you to support legislation that would abolish the Electoral College and elect the President by popular vote. This is the only way to ensure that our elections are truly fair and representative of the will of the people.\\n\\nThank you for your time and consideration.\\n\\nSincerely,\\n\\n[Your Name]\"\n",
    "\n",
    "# preprocess the text\n",
    "human_seq = tokenizer.texts_to_sequences([human_text])\n",
    "human_seq = pad_sequences(human_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "ai_seq = tokenizer.texts_to_sequences([ai_text])\n",
    "ai_seq = pad_sequences(ai_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# make predictions\n",
    "human_pred = model.predict(human_seq)\n",
    "ai_pred = model.predict(ai_seq)\n",
    "\n",
    "print(f\"Human text prediction: {human_pred[0][0]:.4f}\")\n",
    "print(f\"AI text prediction: {ai_pred[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
